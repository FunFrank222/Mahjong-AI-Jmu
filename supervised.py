# éº»å°†AIç›‘ç£å­¦ä¹ è®­ç»ƒè„šæœ¬
# ä½¿ç”¨ä¸“å®¶æ•°æ®è®­ç»ƒCNNæ¨¡å‹å­¦ä¹ éº»å°†å†³ç­–

from dataset import MahjongGBDataset
from torch.utils.data import DataLoader
from model import CNNModel
import torch.nn.functional as F
import torch
from visualizer import MahjongTrainingVisualizer
import os
import shutil

if __name__ == '__main__':
    # è®­ç»ƒæ—¥å¿—ç›®å½•
    logdir = 'log/'
    
    # æ¸…ç†æ—§çš„è®­ç»ƒè®°å½•ï¼Œç¡®ä¿å…¨æ–°å¼€å§‹
    training_log_path = os.path.join(logdir, 'training_log.json')
    visualizations_dir = os.path.join(logdir, 'visualizations')
    
    if os.path.exists(training_log_path):
        os.remove(training_log_path)
        print("å·²æ¸…ç†æ—§çš„è®­ç»ƒæ—¥å¿—")
    
    if os.path.exists(visualizations_dir):
        shutil.rmtree(visualizations_dir)
        print("å·²æ¸…ç†æ—§çš„å¯è§†åŒ–æ–‡ä»¶")
    
    # åˆå§‹åŒ–è®­ç»ƒå¯è§†åŒ–å™¨
    visualizer = MahjongTrainingVisualizer(logdir)
    print("å¼€å§‹å…¨æ–°çš„è®­ç»ƒè¿‡ç¨‹...")
    
    # æ•°æ®é›†åŠ è½½å’Œåˆ’åˆ†
    splitRatio = 0.9  # 90%ç”¨äºè®­ç»ƒï¼Œ10%ç”¨äºéªŒè¯
    batchSize = 1024  # æ‰¹æ¬¡å¤§å°ï¼Œå½±å“è®­ç»ƒé€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨
    
    # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
    trainDataset = MahjongGBDataset(0, splitRatio, True)  # è®­ç»ƒé›†ï¼Œå¯ç”¨æ•°æ®å¢å¼º
    validateDataset = MahjongGBDataset(splitRatio, 1, False)  # éªŒè¯é›†ï¼Œä¸ä½¿ç”¨æ•°æ®å¢å¼º
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    loader = DataLoader(dataset=trainDataset, batch_size=batchSize, shuffle=True)  # è®­ç»ƒæ—¶æ‰“ä¹±æ•°æ®
    vloader = DataLoader(dataset=validateDataset, batch_size=batchSize, shuffle=False)  # éªŒè¯æ—¶ä¸æ‰“ä¹±
    
    # æ¨¡å‹åˆå§‹åŒ–
    model = CNNModel().to('cuda')  # å°†æ¨¡å‹ç§»åˆ°GPUåŠ é€Ÿè®­ç»ƒ
    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)  # Adamä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡5e-4
    
    # æœ€ä½³æ¨¡å‹è·Ÿè¸ªå˜é‡
    best_acc = 0.0      # æœ€ä½³éªŒè¯å‡†ç¡®ç‡
    best_epoch = 0      # æœ€ä½³æ¨¡å‹å¯¹åº”çš„epoch
    best_model_path = '' # æœ€ä½³æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    print("å¼€å§‹è®­ç»ƒ100è½®...")
    
    # ä¸»è®­ç»ƒå¾ªç¯ï¼š100ä¸ªepoch
    for e in range(100):
        print(f'Epoch {e+1}/100')
        
        # æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„
        model_path = logdir + 'checkpoint/%d.pkl' % e
        
        # ç¡®ä¿æ£€æŸ¥ç‚¹ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        # ä¿å­˜å½“å‰epochçš„æ¨¡å‹çŠ¶æ€
        torch.save(model.state_dict(), model_path)
        
        # è®­ç»ƒé˜¶æ®µ
        epoch_losses = []  # è®°å½•å½“å‰epochçš„æ‰€æœ‰æŸå¤±å€¼
        
        for i, d in enumerate(loader):
            # æ„å»ºè¾“å…¥å­—å…¸
            input_dict = {
                'is_training': True,  # è®­ç»ƒæ¨¡å¼
                'obs': {
                    'observation': d[0].cuda(),  # ç‰Œå±€è§‚å¯Ÿ [batch, 38, 4, 9]
                    'action_mask': d[1].cuda()   # åŠ¨ä½œæ©ç  [batch, 235]
                }
            }
            
            # å‰å‘ä¼ æ’­ï¼šè·å–æ¨¡å‹é¢„æµ‹
            logits = model(input_dict)  # [batch, 235]
            
            # è®¡ç®—äº¤å‰ç†µæŸå¤±
            # d[2]æ˜¯ä¸“å®¶åŠ¨ä½œæ ‡ç­¾ï¼Œlogitsæ˜¯æ¨¡å‹é¢„æµ‹çš„åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
            loss = F.cross_entropy(logits, d[2].long().cuda())
            
            # è®°å½•è®­ç»ƒæ­¥éª¤æ•°æ®ç”¨äºå¯è§†åŒ–
            visualizer.log_training_step(e, i, loss.item())
            epoch_losses.append(loss.item())
            
            # æ¯128ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
            if i % 128 == 0:
                print('Iteration %d/%d' % (i, len(trainDataset) // batchSize + 1), 
                      'policy_loss', loss.item())
            
            # åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
            optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
            loss.backward()        # è®¡ç®—æ¢¯åº¦
            optimizer.step()       # æ›´æ–°å‚æ•°
        
        # éªŒè¯é˜¶æ®µ
        print('Run validation:')
        correct = 0  # æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•°
        
        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
        for i, d in enumerate(vloader):
            input_dict = {
                'is_training': False,  # è¯„ä¼°æ¨¡å¼
                'obs': {
                    'observation': d[0].cuda(),
                    'action_mask': d[1].cuda()
                }
            }
            
            # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—
            with torch.no_grad():
                logits = model(input_dict)
                pred = logits.argmax(dim=1)  # è·å–é¢„æµ‹çš„åŠ¨ä½œ
                correct += torch.eq(pred, d[2].cuda()).sum().item()  # ç»Ÿè®¡æ­£ç¡®é¢„æµ‹æ•°
        
        # è®¡ç®—éªŒè¯å‡†ç¡®ç‡
        acc = correct / len(validateDataset)
        print(f'Epoch {e + 1}, éªŒè¯å‡†ç¡®ç‡: {acc:.4f}')
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å†å²æœ€ä½³æ¨¡å‹
        if acc > best_acc:
            best_acc = acc
            best_epoch = e
            best_model_path = model_path
            print(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹ï¼Epoch {e+1}, éªŒè¯å‡†ç¡®ç‡: {acc:.4f}")
        
        # è®°å½•epochæ•°æ®ç”¨äºå¯è§†åŒ–
        visualizer.log_epoch(e, acc)
        
        # æ¯10ä¸ªepochç”Ÿæˆä¸€æ¬¡å¯è§†åŒ–å›¾è¡¨
        if (e + 1) % 10 == 0:
            print(f"æ­£åœ¨ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–... ({e+1}/100)")
            visualizer.plot_training_curves(show=False)
            visualizer.plot_loss_detail(show=False)
            visualizer.create_training_dashboard()
            visualizer.save_training_log()
    
    # è®­ç»ƒå®Œæˆåçš„æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ¯ è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹ä¿¡æ¯ï¼š")
    print(f"ğŸ“Š æœ€ä½³Epoch: {best_epoch + 1}")
    print(f"ğŸ¯ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.4f}")
    print(f"ğŸ“ æœ€ä½³æ¨¡å‹æ–‡ä»¶: {best_model_path}")
    print("="*60)
    
    # å¤åˆ¶æœ€ä½³æ¨¡å‹åˆ°æ ¹ç›®å½•ï¼Œæ–¹ä¾¿éƒ¨ç½²ä½¿ç”¨
    best_model_for_botzone = 'best_model.pkl'
    shutil.copy2(best_model_path, best_model_for_botzone)
    print(f"âœ… æœ€ä½³æ¨¡å‹å·²å¤åˆ¶åˆ°: {best_model_for_botzone}")
    print(f"ğŸš€ è¯·ä½¿ç”¨ {best_model_for_botzone} è¿›è¡ŒBotzoneå¯¹å†³ï¼")
    
    # ç”Ÿæˆæœ€ç»ˆçš„è®­ç»ƒæŠ¥å‘Šå’Œå¯è§†åŒ–
    print("\næ­£åœ¨ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–æŠ¥å‘Š...")
    visualizer.plot_training_curves()
    visualizer.plot_loss_detail()
    visualizer.create_training_dashboard()
    visualizer.save_training_log()
    
    print(f"æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° {logdir}visualizations/ ç›®å½•")
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼ç°åœ¨ä½ å°†è·å¾—å¹²å‡€ç®€æ´çš„å¯è§†åŒ–å›¾åƒï¼")